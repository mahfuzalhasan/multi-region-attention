{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working dirC:\\Users\\abjaw\\Documents\\GitHub\\multi-region-attention\n"
     ]
    }
   ],
   "source": [
    "exec(open(\"init_notebook.py\", \"r\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.encoders.mra_helper import OverlapPatchEmbed, Mlp, DWConv, Block\n",
    "from models.encoders.mra_transformer import MRATransformer\n",
    "from models.decoders.MLPDecoder import DecoderHead\n",
    "from models.encoders.merge_attn import MultiScaleAttention\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlap_patch_embed = OverlapPatchEmbed(patch_size=7, \n",
    "#                                         stride=4, \n",
    "#                                         in_chans=3, \n",
    "#                                         embed_dim=64)\n",
    "\n",
    "# mlp = Mlp(in_features=64,\n",
    "#           hidden_features=64*4,\n",
    "#           act_layer=torch.nn.GELU,\n",
    "#           drop=0)\n",
    "\n",
    "# multiscale_attn = MultiScaleAttention(dim=64,\n",
    "#                                       num_heads=2,\n",
    "#                                       sr_ratio=8,\n",
    "#                                       local_region_shape=[8, 16],\n",
    "#                                       img_size=(256, 256)) \n",
    "\n",
    "# block = Block(dim=64,\n",
    "#               num_heads=2, \n",
    "#               mlp_ratio=4,\n",
    "#               qkv_bias=True,\n",
    "#               qk_scale=None,\n",
    "#               drop=0,\n",
    "#               attn_drop=0, \n",
    "#               drop_path=0,\n",
    "#               norm_layer=torch.nn.LayerNorm,\n",
    "#               sr_ratio=8,\n",
    "#               local_region_shape=[8, 16],\n",
    "#               img_size=(256, 256))\n",
    "\n",
    "\n",
    "mra_transformer = MRATransformer(img_size=(512, 512), patch_size=4, \n",
    "                                   embed_dims=[64, 128, 320, 512], \n",
    "                                   num_heads=[2, 4, 5, 8], \n",
    "                                   mlp_ratios=[4, 4, 4, 4], \n",
    "                                   qkv_bias=True, \n",
    "                                   norm_layer=partial(nn.LayerNorm, eps=1e-6), \n",
    "                                   depths=[3,4,6,3], \n",
    "                                   sr_ratios=[8, 4, 2, 1], \n",
    "                                   drop_rate=0.0, drop_path_rate=0.1)\n",
    "# in_channels=self.channels, num_classes=cfg.num_classes, norm_layer=norm_layer, embed_dim=cfg.decoder_embed_dim\n",
    "mlp_decoder = DecoderHead(in_channels =  [64, 128, 320, 512],\n",
    "                         num_classes = 19,\n",
    "                         norm_layer = nn.BatchNorm2d,\n",
    "                         embed_dim = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of heads  2\n",
      "number of heads  2\n",
      "number of heads  2\n",
      "number of heads  4\n",
      "number of heads  4\n",
      "number of heads  4\n",
      "number of heads  4\n",
      "number of heads  5\n",
      "number of heads  5\n",
      "number of heads  5\n",
      "number of heads  5\n",
      "number of heads  5\n",
      "number of heads  5\n",
      "number of heads  8\n",
      "number of heads  8\n",
      "number of heads  8\n",
      "encoder  5371224064.0\n",
      "mlp  34687942656\n",
      "total  40059166720.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "x_6 = torch.rand(1, 3, 512, 512)\n",
    "y_6 = mra_transformer(x_6)\n",
    "\n",
    "# torch.Size([1, 64, 256, 256]) torch.Size([1, 128, 128, 128]) torch.Size([1, 320, 64, 64]) torch.Size([1, 512, 32, 32])\n",
    "x_7 = [torch.rand(1, 64, 128, 128), torch.rand(1, 128, 64, 64), torch.rand(1, 320, 32, 32), torch.rand(1, 512, 16, 16)]\n",
    "y_7 = mlp_decoder(x_7)\n",
    "\n",
    "\n",
    "mra_flops = mra_transformer.flops()\n",
    "mlp_flops = mlp_decoder.flops()\n",
    "# print(rgbx_transformer.flops())\n",
    "# print(mlp_decoder.flops())\n",
    "\n",
    "print(\"encoder \", mra_flops)\n",
    "print(\"mlp \", mlp_flops)\n",
    "\n",
    "print(\"total \", mra_flops + mlp_flops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
