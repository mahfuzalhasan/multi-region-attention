{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working dir/mnt/d/Study/Research/Segmentation_Local_Attention/scripts/multi-region-attention\n"
     ]
    }
   ],
   "source": [
    "exec(open(\"init_notebook.py\", \"r\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.distributed as dist\n",
    "from torchvision import datasets, transforms\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.data import Mixup\n",
    "from timm.data import create_transform\n",
    "from matplotlib import pyplot as plt\n",
    "from dataloader.imagenet.samplers import SubsetRandomSampler\n",
    "\n",
    "from datasets import load_dataset\n",
    "from dataloader.imagenet.HFDataset import HFDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_loader(config):\n",
    "    # config.defrost()\n",
    "    dataset_train, config.MODEL.NUM_CLASSES = build_dataset(is_train=True, config=config)\n",
    "    # config.freeze()\n",
    "    # print(f\"local rank {config.LOCAL_RANK} / global rank {dist.get_rank()} successfully build train dataset\")\n",
    "    dataset_val, _ = build_dataset(is_train=False, config=config)\n",
    "    # print(f\"local rank {config.LOCAL_RANK} / global rank {dist.get_rank()} successfully build val dataset\")\n",
    "\n",
    "    # num_tasks = dist.get_world_size()\n",
    "    # global_rank = dist.get_rank()\n",
    "    num_tasks = 4\n",
    "    global_rank = 0\n",
    "    if config.DATASET.ZIP_MODE and config.DATASET.CACHE_MODE == 'part':\n",
    "        indices = np.arange(dist.get_rank(), len(dataset_train), dist.get_world_size())\n",
    "        sampler_train = SubsetRandomSampler(indices)\n",
    "    else:\n",
    "        sampler_train = torch.utils.data.DistributedSampler(\n",
    "            dataset_train, num_replicas=num_tasks, rank=global_rank, shuffle=True\n",
    "        )\n",
    "\n",
    "    # indices = np.arange(dist.get_rank(), len(dataset_val), dist.get_world_size())\n",
    "    indices = np.arange(global_rank, len(dataset_val), num_tasks)\n",
    "    sampler_val = SubsetRandomSampler(indices)\n",
    "\n",
    "    # data_loader_train = torch.utils.data.DataLoader(\n",
    "    #     dataset_train, sampler=sampler_train,\n",
    "    #     batch_size=config.DATASET.BATCH_SIZE,\n",
    "    #     num_workers=config.DATASET.NUM_WORKERS,\n",
    "    #     pin_memory=config.DATASET.PIN_MEMORY,\n",
    "    #     drop_last=True,\n",
    "    # )\n",
    "\n",
    "    data_loader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=1,\n",
    "        shuffle=False,  # Set shuffle to False\n",
    "        num_workers=config.DATASET.NUM_WORKERS,\n",
    "        pin_memory=config.DATASET.PIN_MEMORY,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, sampler=sampler_val,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=config.DATASET.NUM_WORKERS,\n",
    "        pin_memory=config.DATASET.PIN_MEMORY,\n",
    "        drop_last=False\n",
    "    )\n",
    "    print(\"dataloader completed\")\n",
    "\n",
    "    # setup mixup / cutmix\n",
    "    mixup_fn = None\n",
    "    mixup_active = config.AUG.MIXUP > 0 or config.AUG.CUTMIX > 0. or config.AUG.CUTMIX_MINMAX is not None\n",
    "    if mixup_active:\n",
    "        mixup_fn = Mixup(\n",
    "            mixup_alpha=config.AUG.MIXUP, cutmix_alpha=config.AUG.CUTMIX, cutmix_minmax=config.AUG.CUTMIX_MINMAX,\n",
    "            prob=config.AUG.MIXUP_PROB, switch_prob=config.AUG.MIXUP_SWITCH_PROB, mode=config.AUG.MIXUP_MODE,\n",
    "            label_smoothing=config.MODEL.LABEL_SMOOTHING, num_classes=config.DATASET.NUM_CLASSES)\n",
    "\n",
    "    return dataset_train, dataset_val, data_loader_train, data_loader_val, mixup_fn\n",
    "\n",
    "def build_dataset(is_train, config):\n",
    "    transform = build_transform(is_train, config)\n",
    "    hf_dataset = load_dataset(\"zh-plus/tiny-imagenet\")\n",
    "\n",
    "    print(\"hf dataset: \", hf_dataset)\n",
    "    if is_train:\n",
    "        hf_dataset = hf_dataset['train']\n",
    "    else:\n",
    "        hf_dataset = hf_dataset['valid']\n",
    "    # Wrap Hugging Face dataset with PyTorch Dataset to apply transformations\n",
    "    dataset = HFDataset(hf_dataset, transform=transform)\n",
    "\n",
    "    nb_classes = 200  # Number of classes for ImageNet\n",
    "    return dataset, nb_classes\n",
    "\n",
    "def build_transform(is_train, config):\n",
    "    resize_im = config.DATASET.IMG_SIZE > 32\n",
    "    if is_train:\n",
    "        # this should always dispatch to transforms_imagenet_train\n",
    "        transform = create_transform(\n",
    "            input_size=config.DATASET.IMG_SIZE,\n",
    "            is_training=True,\n",
    "            color_jitter=config.AUG.COLOR_JITTER if config.AUG.COLOR_JITTER > 0 else None,\n",
    "            auto_augment=config.AUG.AUTO_AUGMENT if config.AUG.AUTO_AUGMENT != 'none' else None,\n",
    "            re_prob=config.AUG.REPROB,\n",
    "            re_mode=config.AUG.REMODE,\n",
    "            re_count=config.AUG.RECOUNT,\n",
    "            interpolation=config.DATASET.INTERPOLATION,\n",
    "        )\n",
    "        if not resize_im:\n",
    "            # replace RandomResizedCropAndInterpolation with\n",
    "            # RandomCrop\n",
    "            transform.transforms[0] = transforms.RandomCrop(config.DATASET.IMG_SIZE, padding=4)\n",
    "        return transform\n",
    "\n",
    "    t = []\n",
    "    if resize_im:\n",
    "        if config.TEST.CROP:\n",
    "            size = int((256 / 224) * config.DATASET.IMG_SIZE)\n",
    "            t.append(\n",
    "                transforms.Resize(size)\n",
    "                # transforms.Resize(size, interpolation=_pil_interp(config.DATASET.INTERPOLATION)),\n",
    "            )\n",
    "            t.append(transforms.CenterCrop(config.DATASET.IMG_SIZE))\n",
    "        else:\n",
    "            t.append(\n",
    "                transforms.Resize((config.DATASET.IMG_SIZE, config.DATASET.IMG_SIZE))\n",
    "            )\n",
    "\n",
    "    t.append(transforms.ToTensor())\n",
    "    t.append(transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD))\n",
    "    return transforms.Compose(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/mahfuz/.cache/huggingface/datasets/zh-plus___parquet/Maysee--tiny-imagenet-fd536421864cbb86/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47cac88b62e6447b950b8d677f78e9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf dataset:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 100000\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/mahfuz/.cache/huggingface/datasets/zh-plus___parquet/Maysee--tiny-imagenet-fd536421864cbb86/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336a7e3cd56d4d22ae0406178410a7dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf dataset:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 100000\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n",
      "dataloader completed\n",
      "\n",
      " #training:100000 #val:10000\n",
      "\n",
      " it in one epoch: len_dl::: train:100000 val:2500\n",
      "\n",
      " batch size:200 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from configs.config_imagenet import config\n",
    "\n",
    "dataset_train, dataset_val, data_loader_train, data_loader_val, mixup_fn = build_loader(config)\n",
    "print(f\"\\n #training:{len(dataset_train)} #val:{len(dataset_val)}\")\n",
    "print(f\"\\n it in one epoch: len_dl::: train:{len(data_loader_train)} val:{len(data_loader_val)}\")\n",
    "print(f\"\\n batch size:{config.DATASET.BATCH_SIZE} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: <class 'int'> \n",
      " samples: <class 'torch.Tensor'> \n",
      " targets: <class 'torch.Tensor'>\n",
      "idx: 0 \n",
      " samples: torch.Size([1, 64, 64, 3]) \n",
      " targets: torch.Size([1])\n",
      "max min:  255 2\n"
     ]
    }
   ],
   "source": [
    "# Assuming you're inside the for loop of your data_loader\n",
    "for idx, (samples, targets) in enumerate(data_loader_train):\n",
    "    print(f\"idx: {type(idx)} \\n samples: {type(samples)} \\n targets: {type(targets)}\")\n",
    "    print(f\"idx: {idx} \\n samples: {samples.shape} \\n targets: {targets.shape}\")\n",
    "\n",
    "    # Select the first image from the batch\n",
    "    image_tensor = samples[0]\n",
    "    # Convert the tensor to PIL Image for displaying\n",
    "    # image_tensor = image_tensor.permute(1, 2, 0)  # Change from (C, H, W) to (H, W, C)\n",
    "    # Convert the tensor to a NumPy array\n",
    "    image_numpy = image_tensor.numpy()\n",
    "    print(\"max min: \",np.max(image_numpy), np.min(image_numpy))\n",
    "\n",
    "    # # Display the image\n",
    "    # plt.imshow(image_numpy)\n",
    "    # plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: <class 'int'> \n",
      " samples: <class 'torch.Tensor'> \n",
      " targets: <class 'torch.Tensor'>\n",
      "idx: 0 \n",
      " samples: torch.Size([1, 64, 64, 3]) \n",
      " targets: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Assuming you're inside the for loop of your data_loader\n",
    "for idx, (samples, targets) in enumerate(data_loader_val):\n",
    "    print(f\"idx: {type(idx)} \\n samples: {type(samples)} \\n targets: {type(targets)}\")\n",
    "    print(f\"idx: {idx} \\n samples: {samples.shape} \\n targets: {targets.shape}\")\n",
    "\n",
    "    # Select the first image from the batch\n",
    "    image_tensor = samples[0]\n",
    "    # Convert the tensor to PIL Image for displaying\n",
    "    image_tensor = image_tensor.permute(1, 2, 0)  # Change from (C, H, W) to (H, W, C)\n",
    "    # Convert the tensor to a NumPy array\n",
    "    image_numpy = image_tensor.numpy()\n",
    "\n",
    "    \n",
    "\n",
    "    # Display the image\n",
    "    # plt.imshow(image_numpy)\n",
    "    # plt.show()\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
